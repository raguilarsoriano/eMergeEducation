{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d90636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from datetime import datetime\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f1b4184",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '0. data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12400\\3597827915.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#df = pd.read_csv(r'C:\\Users\\daniejackson\\Desktop\\eMergeEducation\\Subfolder1\\eMergeEducation')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#df = pd.read_csv(r'C:\\Users\\raguilarsoriano\\OneDrive - Deloitte (O365D)\\1. AI Academy\\eMergeEducation\\0. data.csv')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'0. data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '0. data.csv'"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(r'C:\\Users\\daniejackson\\Desktop\\eMergeEducation\\Subfolder1\\eMergeEducation')\n",
    "#df = pd.read_csv(r'C:\\Users\\raguilarsoriano\\OneDrive - Deloitte (O365D)\\1. AI Academy\\eMergeEducation\\0. data.csv')\n",
    "df = pd.read_csv('0. data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ee646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select column names of object date type\n",
    "sel_cols_float = list(df.select_dtypes(include='float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21142e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select column names of object date type\n",
    "sel_cols_object = list(df.select_dtypes(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cols_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c253c",
   "metadata": {},
   "source": [
    "According to the dataset providers, extensive data preprocessing was conducted to address anomalies, unexplainable outliers, and missing values. Despite this effort, certain transformations are still required to prepare the dataset for the next phase, which involves model-building.\n",
    "\n",
    "Surprisingly, it was discovered that all columns containing categorical data are encoded using a non-linear scale. While the reason for this unconventional encoding remains unknown, the meanings of the numbers are now understood. Therefore, we have decided to convert the existing non-linear scale into a linear one to ensure better interpretability and compatibility with the upcoming modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096dfb1",
   "metadata": {},
   "source": [
    "We must exercise caution when dealing with columns that have non-linear scales and unique values that should remain unchanged. Let's consider two specific examples to illustrate this point.\n",
    "\n",
    "Firstly, the \"grades\" column contains numerical results aggregating student grades. It is crucial that we preserve the original values in this column, as any alteration may lead to inaccurate data representation or misinterpretation of academic performance. Therefore, we will refrain from making any changes to this column.\n",
    "\n",
    "Conversely, we encounter the \"father's qualifications\" column, which represents the level of education attained by fathers. As previously mentioned, the numerical values associated with specific levels of education in this column do not follow a linear scale. To enhance the interpretability of this data and improve its visual representation, we should consider transforming the scale for this column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddb7e59",
   "metadata": {},
   "source": [
    "The code below creates an histogram of every column. The idea is to create an hisitgram just for the column we wnat to change its scale to compare the frequency of the old values vs the new ones after the transformation scale is done as a visual validation that the process was well performed [Better ideas are accepted].   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Get the number of columns and calculate the number of rows needed for the subplots\n",
    "num_cols = df.shape[1]\n",
    "num_rows = (num_cols + 2) // 3  # Ensures at least 3 plots per line\n",
    "\n",
    "# Create subplots with 3 columns\n",
    "fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df.columns):\n",
    "\n",
    "    df[col].value_counts().plot(kind='bar', color='skyblue', edgecolor='black',\n",
    "                                ax=axes[i])\n",
    "    axes[i].set_title(f'Frequency Histogram for {col}')\n",
    "    axes[i].set_xlabel('Unique Values')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove any empty subplots if the number of columns is not divisible by 3\n",
    "for i in range(num_cols, num_rows * 3):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401263f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d57e3a8",
   "metadata": {},
   "source": [
    "The code below changes the scale of the column to iterate defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d4e8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "columns_to_iterate = ['Application mode', 'Application order', 'Course', \n",
    "                      'Nacionality',\"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\",\n",
    "                      \"Father's occupation\"]\n",
    "\n",
    "def replace_values_linear_scale(df):\n",
    "\n",
    "    # Iterate over each column in the dataset\n",
    "    for col_name in columns_to_iterate:\n",
    "        \n",
    "        # Check if the data type of the column is integer\n",
    "        # if df[col_name].dtype == 'int64':\n",
    "            \n",
    "            # Get the unique values in the column\n",
    "            unique_values = df[col_name].unique()\n",
    "\n",
    "            # Sort values in ascending order\n",
    "            unique_values_sorted = sorted(unique_values, reverse=False)\n",
    "\n",
    "            # Count the number of unique values in a column\n",
    "            count = df[col_name].nunique()\n",
    "\n",
    "            # Create a dictionary to store the old value - new value mapping\n",
    "            value_mapping = {}\n",
    "\n",
    "            # Generate a new list for values based on linear scaling\n",
    "            new_values = list(range(1, count + 1))\n",
    "\n",
    "            # Create a mapping of old values to new values based on the linear scale\n",
    "            for i, old_val in enumerate(unique_values_sorted):\n",
    "                new_val = new_values[i]\n",
    "                value_mapping[old_val] = new_val\n",
    "\n",
    "            # Replace values in the dataframe using the mapping\n",
    "            df[col_name] = df[col_name].replace(value_mapping)\n",
    "            \n",
    "    return df\n",
    "replace_values_linear_scale(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08293d2b",
   "metadata": {},
   "source": [
    "#### we need to convert the TARGET column to numeric column to help us find the correlation with others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af90cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To view the hoe the Target valuable looks \n",
    "df['Target'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4af5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can see the three unique values,we can replace them with 0,1,2\n",
    "df['Target'] = df['Target'].map({\n",
    "    'Dropout':0,\n",
    "    'Enrolled':1,\n",
    "    'Graduate':2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791a172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Target column, it must have filled with 0, 1 & 2\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6721d",
   "metadata": {},
   "source": [
    "We create a copy of df named df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d388c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34281817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column in the dataset\n",
    "for col_name in columns_to_iterate:\n",
    "        \n",
    "    # Get the unique values in the column\n",
    "    unique_values = df[col_name].unique()\n",
    "        \n",
    "    # Sort values in ascending order\n",
    "    unique_values_sorted = sorted(unique_values, reverse=False)\n",
    "\n",
    "    # Count the number of unique values in a column\n",
    "    count = df[col_name].nunique()\n",
    "    \n",
    "    # Print the column name and its unique values in ascednding order\n",
    "    print(f\"Column: {col_name}\")\n",
    "    print(count)\n",
    "    print(unique_values_sorted)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532c4e0",
   "metadata": {},
   "source": [
    "## I check the code yesterday and it works. Which means values where changed correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066b35f",
   "metadata": {},
   "source": [
    "### The code below is supposed to ceate histogram for those colum that suffered from scale transformation. Need a few changes, I can do that later. - Robert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac790a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Get the number of columns and calculate the number of rows needed for the subplots\n",
    "num_cols = df.shape[1]\n",
    "num_rows = (num_cols + 2) // 3  # Ensures at least 3 plots per line\n",
    "\n",
    "# Create subplots with 3 columns\n",
    "fig, axes = plt.subplots(num_rows, 3, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Flatten the axes array for easier iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df.columns):\n",
    "    df[col].value_counts().plot(kind='bar', color='orange', edgecolor='black',\n",
    "                                ax=axes[i])\n",
    "    axes[i].set_title(f'Frequency Histogram for {col}')\n",
    "    axes[i].set_xlabel('Unique Values')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Remove any empty subplots if the number of columns is not divisible by 3\n",
    "for i in range(num_cols, num_rows * 3):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4eaee",
   "metadata": {},
   "source": [
    "## EXPLOROTARY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9a097",
   "metadata": {},
   "source": [
    " Exploratory Data Analysis (EDA) is a crucial step in the data analysis process. During EDA, we carefully examine and summarize the main characteristics of our dataset to gain insights, detect patterns, identify potential problems, and formulate hypotheses .describing our data during EDA is fundamental for understanding, cleaning, and preparing the data for analysis. It provides insights into the dataset's characteristics and guides decision-making throughout the data analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e38b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to find out how many dropouts, enrolled  and graduates are there intarget column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaab6ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7cbcc",
   "metadata": {},
   "source": [
    "#### We want to find the correlation of the Target Variable (Target) with other numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe575e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the above values\n",
    "x = df['Target'].value_counts().index\n",
    "y = df['Target'].value_counts().values\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Target': x,\n",
    "    'Count_T' : y\n",
    "})\n",
    "\n",
    "fig = px.pie(df,\n",
    "             names ='Target', \n",
    "             values ='Count_T',\n",
    "            title='How many dropouts, enrolled & graduates are there in Target column')\n",
    "\n",
    "fig.update_traces(labels=['Graduate','Dropout','Enrolled'], hole=0.4,textinfo='value+label', pull=[0,0.2,0.1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6860a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58dc04",
   "metadata": {},
   "source": [
    "### Let's see the correlation of the target with the rest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61cd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.corr()['Target'] # We could create a heat map to visualize all the relations more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7679a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation heatmap\n",
    "dataplot=sns.heatmap(df1.corr())\n",
    "  \n",
    "# displaying heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875318ed",
   "metadata": {},
   "source": [
    "Tuition fees up to date, curricular unist 1st sem (approved),  unist 1st sem (grade), \n",
    "curricular unist 2st sem (approved),  unist 2st sem (grade), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e073c9",
   "metadata": {},
   "source": [
    "##### Let's plot the column Curricular units 2nd sem (approved) againt Curricular units 1st sem (approved) and differentiate Target by color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cef87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df1, \n",
    "             x = 'Curricular units 1st sem (approved)',\n",
    "             y = 'Curricular units 2nd sem (approved)',\n",
    "             color = 'Target')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3429f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df1, \n",
    "             x = 'Curricular units 1st sem (enrolled)',\n",
    "             y = 'Curricular units 2nd sem (enrolled)',\n",
    "             color = 'Target')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4184cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df1, y='Age at enrollment')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97af3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of age of students at the time of enrollment\n",
    "sns.histplot(data=df1['Age at enrollment'], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebccb1",
   "metadata": {},
   "source": [
    "## Model building: Next step is to combine \"enrolled\" class and \"graduated\" class as a single class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802393f5",
   "metadata": {},
   "source": [
    "Given that our dataset covers an entire year of college, we can consider students who remained enrolled as graduates, as both categories essentially denote the same status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93deb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d293e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of old values to new values\n",
    "value_mapping = {\n",
    "    1: 2\n",
    "}\n",
    "\n",
    "# Replace the old values with new values in the specified column\n",
    "df1['Target'].replace(value_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59b78b",
   "metadata": {},
   "source": [
    "## Model building: Next step is to hot-encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa51b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names\n",
    "column_names = df1.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['International'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d9ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on the categorical columns (Category1 and Category2)\n",
    "df1_encoded = pd.get_dummies(df1, columns=['Marital status', 'Application mode', 'Application order', \n",
    "                                           'Course', 'Daytime/evening attendance\\t', 'Previous qualification',\n",
    "                                           'Nacionality', \"Mother's qualification\", \"Father's qualification\", \n",
    "                                           \"Mother's occupation\", \"Father's occupation\",'Displaced', \n",
    "                                           'Educational special needs', 'Debtor', 'Tuition fees up to date', \n",
    "                                           'Gender', 'Scholarship holder', 'Age at enrollment', 'International'\n",
    "                                          ])\n",
    "\n",
    "df1_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43949b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4deea07",
   "metadata": {},
   "source": [
    "## Create training and test sets\n",
    "\n",
    "Before we do anything we'll want to split our data into **_training_** and **_test_** sets.  We'll accomplish this by first splitting the DataFrame into features (`X`) and target (`y`), then passing `X` and `y` to the `train_test_split()` function to split the data so that 70% of it is in the training set, and 30% of it is in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop('Target', axis=1)  # Excluding the target column\n",
    "y = df1['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"oh-encode\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_pipeline = Pipeline(\n",
    "    steps=[(\"impute\", SimpleImputer(strategy=\"mean\")), \n",
    "           (\"scale\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(exclude=\"number\").columns\n",
    "num_cols = X.select_dtypes(include=\"number\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac396c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_processor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_pipeline, num_cols),\n",
    "        (\"categorical\", categorical_pipeline, cat_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36119c57",
   "metadata": {},
   "source": [
    "## Automatic Model Selection with Dabl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b7849",
   "metadata": {},
   "source": [
    "We use a classifier with automatic model selection through the `dabl` library and using our variables (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4439aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fb89545",
   "metadata": {},
   "source": [
    "Note-    Can someone try to install dabl in jupyter notebook? With vs code I couldn't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28138bb8",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088f1db",
   "metadata": {},
   "source": [
    "\n",
    "Naive Bayes falls within the category of supervised learning algorithms known as Bayesian Classification. Employing probability for its predictive analysis, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84286272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# initializaing the NB\n",
    "bayes = BernoulliNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "X_processed = full_processor.fit_transform(X)\n",
    "y_processed = SimpleImputer(strategy=\"most_frequent\").fit_transform(\n",
    "    y.values.reshape(-1, 1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf19fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y_processed, stratify=y_processed, random_state=1121218)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "y_pred = bayes.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33de60b",
   "metadata": {},
   "source": [
    "## Histogram-based Gradient Boosting Classification Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75874b46",
   "metadata": {},
   "source": [
    "Histogram-based gradient boosting is a technique employed in the training of quicker decision trees within the gradient boosting ensemble. This a model that requires a lot of effort while training, but since our dataset is not that big, it will not be a problem. Since we have a Classification type dataset, it is interesting to see the results with this model since it applies gradient boosting combined with decision trees while binning the data.\n",
    "\n",
    "Note-- This is an experimental library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HGB = HistGradientBoostingClassifier(random_state=42, max_leaf_nodes=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f4b48",
   "metadata": {},
   "source": [
    "Basically, the model is binning the data and passing it to the tree when splitting. Anyhow, we do not need to worry for missing values, eventhough the dataset is already cleaned with imputed values.\n",
    "\n",
    "We can start by experimenting with the model with random hyperparameters, and then try prunning until we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca09cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_clf = HistGradientBoostingClassifier(loss='binary_crossentropy',   \n",
    "                                        learning_rate=0.1,          # regulates the contribution of each tree\n",
    "                                        max_iter=150,               \n",
    "                                        min_samples_leaf=15,\n",
    "                                        max_depth=None,\n",
    "                                        random_state=None,\n",
    "                                        max_leaf_nodes=35           \n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5064afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_clf.fit(X_train, y_train)\n",
    "hgb_preds = hgb_clf.predict(X_test)\n",
    "accuracy_score(y_test, hgb_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2451d",
   "metadata": {},
   "source": [
    "#### Prunning the Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f9b0c",
   "metadata": {},
   "source": [
    "We can start by adding an evaluation procedure to our model. We will use Grid Search technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15572bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3caa03",
   "metadata": {},
   "source": [
    "We will find the best value for our max iterations and evaluate with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": (0.01, 0.1, 1, 10,100),\n",
    "    \"max_leaf_nodes\": (3, 10, 30,40),\n",
    "    \"max_iter\": (10, 20, 50, 100, 300, 500),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search = GridSearchCV(HGB, param_grid=param_grid, n_jobs=2, cv=2)\n",
    "model_grid_search.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9784fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = model_grid_search.score(X_test, y_test)\n",
    "print(\n",
    "    f\"Accuracy score of the grid-search pipeline is of: {accuracy:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fefc3",
   "metadata": {},
   "source": [
    "We should be careful with this accuracy, it would be better to apply a nested cross validation to evaluate our grid search, and then our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of parameters is: {model_grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c64d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_clf_tuned = HistGradientBoostingClassifier(loss='binary_crossentropy',   \n",
    "                                        learning_rate=0.01,          #apply the best parameters to our model\n",
    "                                        max_iter=500,               \n",
    "                                        min_samples_leaf=15,\n",
    "                                        max_depth=None,\n",
    "                                        random_state=None,\n",
    "                                        max_leaf_nodes=10           \n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4084123",
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_clf_tuned.fit(X_train, y_train)\n",
    "hgb_tuned_preds = hgb_clf_tuned.predict(X_test)\n",
    "accuracy_score(y_test, hgb_tuned_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7062e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search.predict(X_test[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54b699",
   "metadata": {},
   "source": [
    "Let's evaluate the behavior with other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea5d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv_results = pd.DataFrame(model_grid_search.cv_results_).sort_values(\n",
    "    \"mean_test_score\", ascending=False\n",
    ")\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca848e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we are getting the parameters names\n",
    "column_results = [f\"param_{name}\" for name in param_grid.keys()]\n",
    "column_results += [\n",
    "    \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]\n",
    "cv_results = cv_results[column_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce322647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_param(param_name):\n",
    "    if \"__\" in param_name:\n",
    "        return param_name.rsplit(\"__\", 1)[1]\n",
    "    return param_name\n",
    "\n",
    "\n",
    "cv_results = cv_results.rename(shorten_param, axis=1)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb356ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pivoted_cv_results = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\",\n",
    "    index=[\"param_learning_rate\"],\n",
    "    columns=[\"param_max_leaf_nodes\"],\n",
    ")\n",
    "\n",
    "pivoted_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a975215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    pivoted_cv_results, annot=True, cmap=\"YlGnBu\", vmin=0.7, vmax=0.9\n",
    ")\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77318702",
   "metadata": {},
   "source": [
    "When we have a higher value in the learning rate parameter our model basically gets an accuracy of most of our baseline model, even while using hyperparameter.\n",
    "\n",
    "It is interesting to see that this accuracy decreases exponencially if both learning rate and max_leaf_nodes are high, giving an accuracy of just 33%.\n",
    "\n",
    "But then, when we have a learning rate of just 0.1, then the change in the hyperparameter of max_leaf_nodes does not really make a difference, event though it increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6eef8",
   "metadata": {},
   "source": [
    "# Extra stuff, if not used we´ll remove it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7181bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# print(datetime.now() - startTime)\n",
    "print('It took', time.time()-start, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model building: Next step is resampling  to address imbalanced classes. \n",
    "\n",
    "----Note: Do we want to give certain weight to each class? For example, do we give more importance to enrolled and dropout instead of graduate, or do we give 0 weight to all of the classes, I think this may be a good question for Stephen. I will include notes from our classes:\n",
    "\n",
    "                                     \n",
    "\n",
    "Let's look a the level of class imbalance in the dataset\n",
    "\n",
    "print('Raw counts: \\n')\n",
    "print(df['Target'].value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(df['Target'].value_counts(normalize=True))\n",
    "\n",
    "Based on the results, we can see that only 17.94% of the data corresponds to students with an \"Enrolled\" status\n",
    "\n",
    "# Define appropriate X and y\n",
    "y = df['Target']\n",
    "X = df.drop('Target', axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Previous original class distribution\n",
    "print('Original class distribution: \\n')\n",
    "print(y.value_counts())\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) \n",
    "# Preview synthetic sample class distribution\n",
    "print('-----------------------------------------')\n",
    "print('Synthetic sample class distribution: \\n')\n",
    "print(pd.Series(y_train_resampled).value_counts()) \n",
    "\n",
    "-\n",
    "\n",
    "print('Raw counts: \\n')\n",
    "print(y_test.value_counts())\n",
    "print('-----------------------------------')\n",
    "print('Normalized counts: \\n')\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de85ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
